<!doctype html><html lang=zh-cn><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="noodp"><title>Python实现11种特征选择策略 - QiMington's</title><meta name=keywords content="贺兴宇,QiMington,启明顿,个人博客,blog.qimington.com"><meta name=description content="我在这里分享一些有趣的东西，寻找志同道合的朋友。I am here to share some interesting things and find like-minded friends."><meta property="og:title" content="Python实现11种特征选择策略"><meta property="og:description" content="Python实现11种特征选择策略 太多的特征会增加模型的复杂性和过拟合，而太少的特征会导致模型的拟合不足。将模型优化为足够复杂以使其性能可推广，但又足够简单易于训练、维护和解释是特征选择的主要工作。
“特征选择”意味着可以保留一些特征并放弃其他一些特征。本文的目的是概述一些特征选择策略：
  删除未使用的列
  删除具有缺失值的列
  不相关的特征
  低方差特征
  多重共线性
  特征系数
  p 值
  方差膨胀因子 (VIF)
  基于特征重要性的特征选择
  使用 scikit-learn 进行自动特征选择
  主成分分析 (PCA )
  该演示的数据集在 MIT 许可下发布，来自 PyCaret——一个开源的低代码机器学习库。
数据集相当干净，但我做了一些预处理。请注意，我使用此数据集来演示不同的特征选择策略如何工作，而不是构建最终模型，因此模型性能无关紧要。
首先加载数据集：
1 2 3 4  import pandas as pddata = 'https://raw.githubusercontent.com/pycaret/pycaret/master/datasets/automobile.csv'df = pd.read_csv(data) # remove rows with '?'df = df[df['bore']!='?']df = df[df['stroke']!"><meta property="og:type" content="article"><meta property="og:url" content="/posts/ml/"><meta property="article:published_time" content="2022-05-20T10:50:40+08:00"><meta property="article:modified_time" content="2022-05-20T10:50:40+08:00"><meta property="og:site_name" content="QiMington's"><meta name=twitter:card content="summary"><meta name=twitter:title content="Python实现11种特征选择策略"><meta name=twitter:description content="Python实现11种特征选择策略 太多的特征会增加模型的复杂性和过拟合，而太少的特征会导致模型的拟合不足。将模型优化为足够复杂以使其性能可推广，但又足够简单易于训练、维护和解释是特征选择的主要工作。
“特征选择”意味着可以保留一些特征并放弃其他一些特征。本文的目的是概述一些特征选择策略：
  删除未使用的列
  删除具有缺失值的列
  不相关的特征
  低方差特征
  多重共线性
  特征系数
  p 值
  方差膨胀因子 (VIF)
  基于特征重要性的特征选择
  使用 scikit-learn 进行自动特征选择
  主成分分析 (PCA )
  该演示的数据集在 MIT 许可下发布，来自 PyCaret——一个开源的低代码机器学习库。
数据集相当干净，但我做了一些预处理。请注意，我使用此数据集来演示不同的特征选择策略如何工作，而不是构建最终模型，因此模型性能无关紧要。
首先加载数据集：
1 2 3 4  import pandas as pddata = 'https://raw.githubusercontent.com/pycaret/pycaret/master/datasets/automobile.csv'df = pd.read_csv(data) # remove rows with '?'df = df[df['bore']!='?']df = df[df['stroke']!"><meta name=application-name content="QiMington's"><meta name=apple-mobile-web-app-title content="QiMington's"><meta name=theme-color content="#ffffff"><meta name=msapplication-TileColor content="#da532c"><link rel=icon href=/avatar.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical href=/posts/ml/><link rel=next href=/posts/redis/redis%E5%85%A5%E9%97%A8/><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/normalize.css@8.0.1/normalize.min.css><link rel=stylesheet href=/css/style.min.css><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css><meta name=google-site-verification content="gKS_lOzqrL6dS6WciO5_y1FmkXkuK6581tNNUSFvYq4"><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"Python实现11种特征选择策略","inLanguage":"zh-CN","mainEntityOfPage":{"@type":"WebPage","@id":"\/posts\/ml\/"},"genre":"posts","keywords":"Python, 特征选择","wordcount":792,"url":"\/posts\/ml\/","datePublished":"2022-05-20T10:50:40+08:00","dateModified":"2022-05-20T10:50:40+08:00","publisher":{"@type":"Organization","name":""},"author":{"@type":"Person","name":"QiMington"},"description":""}</script></head><body data-header-desktop=fixed data-header-mobile=auto><script type=text/javascript>(window.localStorage&&localStorage.getItem('theme')?localStorage.getItem('theme')==='dark':('auto'==='auto'?window.matchMedia('(prefers-color-scheme: dark)').matches:'auto'==='dark'))&&document.body.setAttribute('theme','dark');</script><div id=mask></div><div class=wrapper><header class=desktop id=header-desktop><div class=header-wrapper><div class=header-title><a href=/ title="QiMington's">QiMington</a></div><div class=menu><div class=menu-inner><a class=menu-item href=/>首页 </a><a class=menu-item href=/posts/>所有文章 </a><a class=menu-item href=/tags/>标签 </a><a class=menu-item href=/categories/>分类 </a><a class=menu-item href=/about/>关于 </a><span class="menu-item delimiter"></span><a href=javascript:void(0); class="menu-item theme-switch" title=切换主题><i class="fas fa-adjust fa-fw" aria-hidden=true></i></a></div></div></div></header><header class=mobile id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title="QiMington's">QiMington</a></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><div class=menu id=menu-mobile><a class=menu-item href=/>首页</a><a class=menu-item href=/posts/>所有文章</a><a class=menu-item href=/tags/>标签</a><a class=menu-item href=/categories/>分类</a><a class=menu-item href=/about/>关于</a><a href=javascript:void(0); class="menu-item theme-switch" title=切换主题>
<i class="fas fa-adjust fa-fw" aria-hidden=true></i></a></div></div></header><main class=main><div class=container><div class=toc id=toc-auto><h2 class=toc-title>目录</h2><div class=toc-content id=toc-content-auto></div></div><article class="page single"><h1 class="single-title animate__animated animate__flipInX">Python实现11种特征选择策略</h1><div class=post-meta><div class=post-meta-line><span class=post-author><a href=/ title=Author rel=author class=author><i class="fas fa-user-circle fa-fw" aria-hidden=true></i>QiMington</a>
</span>&nbsp;<span class=post-category>收录于 <a href=/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/><i class="far fa-folder fa-fw" aria-hidden=true></i>机器学习</a></span></div><div class=post-meta-line><i class="far fa-calendar-alt fa-fw" aria-hidden=true></i>&nbsp;<time datetime=2022-05-20>2022-05-20</time>&nbsp;<i class="fas fa-pencil-alt fa-fw" aria-hidden=true></i>&nbsp;约 792 字&nbsp;
<i class="far fa-clock fa-fw" aria-hidden=true></i>&nbsp;预计阅读 4 分钟&nbsp;</div></div><div class="details toc" id=toc-static data-kept><div class="details-summary toc-title"><span>目录</span>
<span><i class="details-icon fas fa-angle-right" aria-hidden=true></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#1-删除未使用的列>1 删除未使用的列</a></li><li><a href=#2-删除具有缺失值的列>2 删除具有缺失值的列</a></li><li><a href=#3-不相关的特征>3 不相关的特征</a></li><li><a href=#4-低方差特征>4 低方差特征</a></li><li><a href=#5-多重共线性>5 多重共线性</a></li><li><a href=#6-特征系数>6 特征系数</a></li><li><a href=#7-p-值>7 p 值</a></li><li><a href=#8-方差膨胀因子-vif>8 方差膨胀因子 (VIF)</a></li><li><a href=#9-基于特征重要性选择>9 基于特征重要性选择</a></li><li><a href=#10-使用-scikit-learn-自动选择特征>10 使用 Scikit Learn 自动选择特征</a></li><li><a href=#总结>总结</a></li></ul></nav></div></div><div class=content id=content><h1 id=python实现11种特征选择策略>Python实现11种特征选择策略</h1><p><strong>太多的特征</strong>会增加模型的<strong>复杂性和过拟合</strong>，而<strong>太少的特征</strong>会导致模型的<strong>拟合不足</strong>。将模型优化为足够复杂以使其性能可推广，但又足够简单易于训练、维护和解释是特征选择的主要工作。</p><p>“特征选择”意味着可以<strong>保留一些特征</strong>并<strong>放弃其他一些特征</strong>。本文的目的是概述一些特征选择策略：</p><ol><li><p><strong>删除未使用</strong>的列</p></li><li><p><strong>删除</strong>具有<strong>缺失值</strong>的列</p></li><li><p><strong>不相关</strong>的特征</p></li><li><p><strong>低方差</strong>特征</p></li><li><p><strong>多重共线性</strong></p></li><li><p>特征<strong>系数</strong></p></li><li><p><strong>p</strong> 值</p></li><li><p><strong>方差膨胀因子</strong> (VIF)</p></li><li><p>基于<strong>特征重要性</strong>的特征选择</p></li><li><p>使用 scikit-learn 进行<strong>自动特征选择</strong></p></li><li><p>主成分分析 (<a href="http://mp.weixin.qq.com/s?__biz=MzI4ODgwMjYyNQ==&mid=2247483848&idx=1&sn=7cfd6d06e94e4a85abf276cf345c15e4&chksm=ec3993e6db4e1af068abc956f5b61e4e933ffe1779394452228f284afa251ebbb36db156ae18&scene=21#wechat_redirect" target=_blank rel="noopener noreffer"><strong>PCA</strong></a>
)</p></li></ol><p>该演示的数据集在 MIT 许可下发布，来自 PyCaret——一个开源的低代码机器学习库。</p><p>数据集相当干净，但我做了一些预处理。请注意，我使用此数据集来演示不同的特征选择策略如何工作，而不是构建最终模型，因此模型性能无关紧要。</p><p>首先加载数据集：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-python data-lang=python><span class=kn>import</span> <span class=nn>pandas</span> <span class=kn>as</span> <span class=nn>pddata</span> <span class=o>=</span> <span class=sa></span><span class=s1>&#39;</span><span class=s1>https://raw.githubusercontent.com/pycaret/pycaret/master/datasets/automobile.csv</span><span class=s1>&#39;</span><span class=n>df</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>read_csv</span><span class=p>(</span><span class=n>data</span><span class=p>)</span>
<span class=c1># remove rows with &#39;?&#39;df = df[df[&#39;bore&#39;]!=&#39;?&#39;]df = df[df[&#39;stroke&#39;]!=&#39;?&#39;]df = df[df[&#39;horsepower&#39;]!=&#39;?&#39;]df = df[df[&#39;peak-rpm&#39;]!=&#39;?&#39;]</span>
<span class=c1># convert data typedf[&#39;bore&#39;] = df[&#39;bore&#39;].astype(&#39;float&#39;)df[&#39;stroke&#39;] = df[&#39;stroke&#39;].astype(&#39;float&#39;)df[&#39;horsepower&#39;] = df[&#39;horsepower&#39;].astype(&#39;int&#39;)df[&#39;peak-rpm&#39;] = df[&#39;peak-rpm&#39;].astype(&#39;int&#39;)cat_cols = df.select_dtypes(include=&#39;O&#39;).columns.to_list()df[cat_cols] = df[cat_cols].astype(&#39;category&#39;)</span>
<span class=n>df</span><span class=o>.</span><span class=n>sample</span><span class=p>(</span><span class=mi>5</span><span class=p>)</span>
</code></pre></td></tr></table></div></div><p><img class=lazyload src=/svg/loading.min.svg data-src=https://cdn.jsdelivr.net/gh/QiMington/picbed/640.png data-srcset="https://cdn.jsdelivr.net/gh/QiMington/picbed/640.png, https://cdn.jsdelivr.net/gh/QiMington/picbed/640.png 1.5x, https://cdn.jsdelivr.net/gh/QiMington/picbed/640.png 2x" data-sizes=auto alt=https://cdn.jsdelivr.net/gh/QiMington/picbed/640.png title=图片></p><p>该数据集包含 <strong>202 行</strong>和 <strong>26 列</strong>——<strong>每行代表一个汽车实例</strong>，每列代表其<strong>特征和相应的价格</strong>。这些列包括：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-python data-lang=python><span class=n>df</span><span class=o>.</span><span class=n>columns</span>
<span class=o>&gt;&gt;</span><span class=o>&gt;</span> <span class=n>Index</span><span class=p>(</span><span class=p>[</span><span class=sa></span><span class=s1>&#39;</span><span class=s1>symboling</span><span class=s1>&#39;</span><span class=p>,</span> <span class=sa></span><span class=s1>&#39;</span><span class=s1>normalized-losses</span><span class=s1>&#39;</span><span class=p>,</span> <span class=sa></span><span class=s1>&#39;</span><span class=s1>make</span><span class=s1>&#39;</span><span class=p>,</span> <span class=sa></span><span class=s1>&#39;</span><span class=s1>fuel-type</span><span class=s1>&#39;</span><span class=p>,</span> <span class=sa></span><span class=s1>&#39;</span><span class=s1>aspiration</span><span class=s1>&#39;</span><span class=p>,</span> <span class=sa></span><span class=s1>&#39;</span><span class=s1>num-of-doors</span><span class=s1>&#39;</span><span class=p>,</span> <span class=sa></span><span class=s1>&#39;</span><span class=s1>body-style</span><span class=s1>&#39;</span><span class=p>,</span> <span class=sa></span><span class=s1>&#39;</span><span class=s1>drive-wheels</span><span class=s1>&#39;</span><span class=p>,</span> <span class=sa></span><span class=s1>&#39;</span><span class=s1>engine-location</span><span class=s1>&#39;</span><span class=p>,</span><span class=sa></span><span class=s1>&#39;</span><span class=s1>wheel-base</span><span class=s1>&#39;</span><span class=p>,</span> <span class=sa></span><span class=s1>&#39;</span><span class=s1>length</span><span class=s1>&#39;</span><span class=p>,</span> <span class=sa></span><span class=s1>&#39;</span><span class=s1>width</span><span class=s1>&#39;</span><span class=p>,</span> <span class=sa></span><span class=s1>&#39;</span><span class=s1>height</span><span class=s1>&#39;</span><span class=p>,</span> <span class=sa></span><span class=s1>&#39;</span><span class=s1>curb-weight</span><span class=s1>&#39;</span><span class=p>,</span> <span class=sa></span><span class=s1>&#39;</span><span class=s1>engine-type</span><span class=s1>&#39;</span><span class=p>,</span> <span class=sa></span><span class=s1>&#39;</span><span class=s1>num-of-cylinders</span><span class=s1>&#39;</span><span class=p>,</span> <span class=sa></span><span class=s1>&#39;</span><span class=s1>engine-size</span><span class=s1>&#39;</span><span class=p>,</span> <span class=sa></span><span class=s1>&#39;</span><span class=s1>fuel-system</span><span class=s1>&#39;</span><span class=p>,</span> <span class=sa></span><span class=s1>&#39;</span><span class=s1>bore</span><span class=s1>&#39;</span><span class=p>,</span> <span class=sa></span><span class=s1>&#39;</span><span class=s1>stroke</span><span class=s1>&#39;</span><span class=p>,</span> <span class=sa></span><span class=s1>&#39;</span><span class=s1>compression-ratio</span><span class=s1>&#39;</span><span class=p>,</span> <span class=sa></span><span class=s1>&#39;</span><span class=s1>horsepower</span><span class=s1>&#39;</span><span class=p>,</span> <span class=sa></span><span class=s1>&#39;</span><span class=s1>peak-rpm</span><span class=s1>&#39;</span><span class=p>,</span> <span class=sa></span><span class=s1>&#39;</span><span class=s1>city-mpg</span><span class=s1>&#39;</span><span class=p>,</span> <span class=sa></span><span class=s1>&#39;</span><span class=s1>highway-mpg</span><span class=s1>&#39;</span><span class=p>,</span> <span class=sa></span><span class=s1>&#39;</span><span class=s1>price</span><span class=s1>&#39;</span><span class=p>]</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=sa></span><span class=s1>&#39;</span><span class=s1>object</span><span class=s1>&#39;</span><span class=p>)</span>
</code></pre></td></tr></table></div></div><p>现在让我们深入研究特征选择的 11 种策略。</p><h2 id=1-删除未使用的列><strong>1 删除未使用的列</strong></h2><p>当然，最简单的策略是你的直觉。虽然是直觉，但有时很有用的，<strong>某些列在最终模型中不会以任何形式使用</strong>（例如“ID”、“FirstName”、“LastName”等列）。如果您知道某个特定列将不会被使用，请随时将其删除。在我们的数据中，没有一列有这样的问题所以，我在此步骤中不删除任何列。</p><h2 id=2-删除具有缺失值的列><strong>2 删除具有缺失值的列</strong></h2><p>缺失值在机器学习中是不可接受的，因此我们会采用不同的策略来清理缺失数据（例如插补）。但是如果<strong>列中缺少大量数据</strong>，那么完全删除它是非常好的方法。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-python data-lang=python><span class=c1># total null values per columndf.isnull().sum()</span>
</code></pre></td></tr></table></div></div><p><img class=lazyload src=/svg/loading.min.svg data-src=https://cdn.jsdelivr.net/gh/QiMington/picbed/640-20231228133640170.png data-srcset="https://cdn.jsdelivr.net/gh/QiMington/picbed/640-20231228133640170.png, https://cdn.jsdelivr.net/gh/QiMington/picbed/640-20231228133640170.png 1.5x, https://cdn.jsdelivr.net/gh/QiMington/picbed/640-20231228133640170.png 2x" data-sizes=auto alt=https://cdn.jsdelivr.net/gh/QiMington/picbed/640-20231228133640170.png title=图片></p><h2 id=3-不相关的特征><strong>3 不相关的特征</strong></h2><p>无论算法是<strong>回归（预测数字）<strong>还是</strong>分类（预测类别）</strong>，特征都必须与目标相关。如果一个特征没有表现出相关性，它就是一个主要的消除目标。可以<strong>分别测试数值和分类特征的相关性</strong>。</p><p><strong>1) 数值变量</strong></p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-python data-lang=python><span class=c1># correlation between target and featuresdf.corr().loc[&#39;price&#39;].plot(kind=&#39;barh&#39;, figsize=(4,10))</span>
</code></pre></td></tr></table></div></div><p><img class=lazyload src=/svg/loading.min.svg data-src=https://cdn.jsdelivr.net/gh/QiMington/picbed/640-20231228133643662.png data-srcset="https://cdn.jsdelivr.net/gh/QiMington/picbed/640-20231228133643662.png, https://cdn.jsdelivr.net/gh/QiMington/picbed/640-20231228133643662.png 1.5x, https://cdn.jsdelivr.net/gh/QiMington/picbed/640-20231228133643662.png 2x" data-sizes=auto alt=https://cdn.jsdelivr.net/gh/QiMington/picbed/640-20231228133643662.png title=图片></p><p>在此示例中，peak-rpm, compression-ratio, stroke, bore, height , symboling 等特征<strong>与价格几乎没有相关性</strong>，因此我们可以删除它们。</p><p>可以<strong>手动删除列</strong>，但我更喜欢<strong>使用相关阈值</strong>（在本例中为 <strong>0.2</strong>）以编程方式进行：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-python data-lang=python><span class=c1># drop uncorrelated numeric features (threshold &lt;0.2)corr = abs(df.corr().loc[&#39;price&#39;])corr = corr[corr&lt;0.2]cols_to_drop = corr.index.to_list()df = df.drop(cols_to_drop, axis=1)</span>
</code></pre></td></tr></table></div></div><p><strong>2) 分类变量</strong></p><p>可以使用<strong>箱线图</strong>查找目标和分类特征之间的相关性：</p><pre><code>import seaborn as sns
sns.boxplot(y = 'price', x = 'fuel-type', data=df)
</code></pre><p><img class=lazyload src=/svg/loading.min.svg data-src=https://cdn.jsdelivr.net/gh/QiMington/picbed/640-20231228133652504.png data-srcset="https://cdn.jsdelivr.net/gh/QiMington/picbed/640-20231228133652504.png, https://cdn.jsdelivr.net/gh/QiMington/picbed/640-20231228133652504.png 1.5x, https://cdn.jsdelivr.net/gh/QiMington/picbed/640-20231228133652504.png 2x" data-sizes=auto alt=https://cdn.jsdelivr.net/gh/QiMington/picbed/640-20231228133652504.png title=图片></p><p><strong>柴油车的中位价高于汽油车</strong>。这意味着这个分类变量可以解释汽车价格，所以应放弃它。可以像这样单独检查每个分类列。</p><h2 id=4-低方差特征><strong>4 低方差特征</strong></h2><p>检查一下我们的<strong>特征的差异</strong>：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-python data-lang=python><span class=kn>import</span> <span class=nn>numpy</span> <span class=kn>as</span> <span class=nn>np</span>
<span class=c1># variance of numeric featuresdf.select_dtypes(include=np.number).var().astype(&#39;str&#39;)</span>
</code></pre></td></tr></table></div></div><p><img class=lazyload src=/svg/loading.min.svg data-src=https://cdn.jsdelivr.net/gh/QiMington/picbed/640-20231228133700375.png data-srcset="https://cdn.jsdelivr.net/gh/QiMington/picbed/640-20231228133700375.png, https://cdn.jsdelivr.net/gh/QiMington/picbed/640-20231228133700375.png 1.5x, https://cdn.jsdelivr.net/gh/QiMington/picbed/640-20231228133700375.png 2x" data-sizes=auto alt=https://cdn.jsdelivr.net/gh/QiMington/picbed/640-20231228133700375.png title=图片></p><p>这里的“<strong>bore</strong>”具有<strong>极低的方差</strong>，虽然这是<strong>删除的候选者</strong>。在这个特殊的例子中，我不愿意删除它，因为它的<strong>值在2.54和3.94之间</strong>，因此方差很低：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-python data-lang=python><span class=n>df</span><span class=p>[</span><span class=sa></span><span class=s1>&#39;</span><span class=s1>bore</span><span class=s1>&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>describe</span><span class=p>(</span><span class=p>)</span>
</code></pre></td></tr></table></div></div><p><img class=lazyload src=/svg/loading.min.svg data-src=https://cdn.jsdelivr.net/gh/QiMington/picbed/640-20231228133710059.png data-srcset="https://cdn.jsdelivr.net/gh/QiMington/picbed/640-20231228133710059.png, https://cdn.jsdelivr.net/gh/QiMington/picbed/640-20231228133710059.png 1.5x, https://cdn.jsdelivr.net/gh/QiMington/picbed/640-20231228133710059.png 2x" data-sizes=auto alt=https://cdn.jsdelivr.net/gh/QiMington/picbed/640-20231228133710059.png title=图片></p><h2 id=5-多重共线性><strong>5 多重共线性</strong></h2><p>当任何<strong>两个特征之间存在相关性</strong>时，就会出现<strong>多重共线性</strong>。在机器学习中，期望每个特征都应该<strong>独立于其他特征</strong>，即它们之间<strong>没有共线性</strong>。高马力车辆往往具有高发动机尺寸。所以你可能想消除其中一个，让另一个决定目标变量——价格。</p><p>我们可以<strong>分别测试数字和分类特征</strong>的多重共线性：</p><p><strong>1) 数值变量</strong></p><p><strong>Heatmap</strong> 是检查和寻找相关特征的最简单方法。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-python data-lang=python><span class=kn>import</span> <span class=nn>matplotlib.pyplot</span> <span class=kn>as</span> <span class=nn>plt</span>
<span class=n>sns</span><span class=o>.</span><span class=n>set</span><span class=p>(</span><span class=n>rc</span><span class=o>=</span><span class=p>{</span><span class=sa></span><span class=s1>&#39;</span><span class=s1>figure.figsize</span><span class=s1>&#39;</span><span class=p>:</span><span class=p>(</span><span class=mi>16</span><span class=p>,</span><span class=mi>10</span><span class=p>)</span><span class=p>}</span><span class=p>)</span><span class=n>sns</span><span class=o>.</span><span class=n>heatmap</span><span class=p>(</span><span class=n>df</span><span class=o>.</span><span class=n>corr</span><span class=p>(</span><span class=p>)</span><span class=p>,</span> <span class=n>annot</span><span class=o>=</span><span class=bp>True</span><span class=p>,</span><span class=n>linewidths</span><span class=o>=</span><span class=o>.</span><span class=mi>5</span><span class=p>,</span><span class=n>center</span><span class=o>=</span><span class=mi>0</span><span class=p>,</span><span class=n>cbar</span><span class=o>=</span><span class=bp>False</span><span class=p>,</span><span class=n>cmap</span><span class=o>=</span><span class=sa></span><span class=s2>&#34;</span><span class=s2>PiYG</span><span class=s2>&#34;</span><span class=p>)</span><span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>(</span><span class=p>)</span>
</code></pre></td></tr></table></div></div><p><img class=lazyload src=/svg/loading.min.svg data-src=https://cdn.jsdelivr.net/gh/QiMington/picbed/640-20231228133715518.png data-srcset="https://cdn.jsdelivr.net/gh/QiMington/picbed/640-20231228133715518.png, https://cdn.jsdelivr.net/gh/QiMington/picbed/640-20231228133715518.png 1.5x, https://cdn.jsdelivr.net/gh/QiMington/picbed/640-20231228133715518.png 2x" data-sizes=auto alt=https://cdn.jsdelivr.net/gh/QiMington/picbed/640-20231228133715518.png title=图片></p><p>大多数特征在某种程度上相互关联，但有些特征具有非常高的相关性，例如<strong>长度与轴距</strong>以及<strong>发动机尺寸与马力</strong>。</p><p>可以根据<strong>相关阈值</strong>手动或以编程方式删除这些功能。我将<strong>手动删除具有 0.80 共线性阈值的特征</strong>。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-python data-lang=python><span class=c1># drop correlated featuresdf = df.drop([&#39;length&#39;, &#39;width&#39;, &#39;curb-weight&#39;, &#39;engine-size&#39;, &#39;city-mpg&#39;], axis=1)</span>
</code></pre></td></tr></table></div></div><p>还可以使用称为<strong>方差膨胀因子 (VIF)</strong> 的方法来确定多重共线性并根据高 VIF 值删除特征。我稍后会展示这个例子。</p><p><strong>2) 分类变量</strong></p><p>与数值特征类似，也可以检查分类变量之间的共线性。诸如<strong>独立性卡方检验</strong>之类的统计检验非常适合它。</p><p>让我们检查一下数据集中的两个分类列——<strong>燃料类型和车身风格</strong>——是独立的还是相关的。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-python data-lang=python><span class=n>df_cat</span> <span class=o>=</span> <span class=n>df</span><span class=p>[</span><span class=p>[</span><span class=sa></span><span class=s1>&#39;</span><span class=s1>fuel-type</span><span class=s1>&#39;</span><span class=p>,</span> <span class=sa></span><span class=s1>&#39;</span><span class=s1>body-style</span><span class=s1>&#39;</span><span class=p>]</span><span class=p>]</span><span class=n>df_cat</span><span class=o>.</span><span class=n>sample</span><span class=p>(</span><span class=mi>5</span><span class=p>)</span>
</code></pre></td></tr></table></div></div><p><img class=lazyload src=/svg/loading.min.svg data-src=https://cdn.jsdelivr.net/gh/QiMington/picbed/640-20231228133720479.png data-srcset="https://cdn.jsdelivr.net/gh/QiMington/picbed/640-20231228133720479.png, https://cdn.jsdelivr.net/gh/QiMington/picbed/640-20231228133720479.png 1.5x, https://cdn.jsdelivr.net/gh/QiMington/picbed/640-20231228133720479.png 2x" data-sizes=auto alt=https://cdn.jsdelivr.net/gh/QiMington/picbed/640-20231228133720479.png title=图片></p><p>然后我们将在每一列中创建一个类别的<strong>交叉表/列联表</strong>。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-python data-lang=python><span class=n>crosstab</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>crosstab</span><span class=p>(</span><span class=n>df_cat</span><span class=p>[</span><span class=sa></span><span class=s1>&#39;</span><span class=s1>fuel-type</span><span class=s1>&#39;</span><span class=p>]</span><span class=p>,</span> <span class=n>df_cat</span><span class=p>[</span><span class=sa></span><span class=s1>&#39;</span><span class=s1>body-style</span><span class=s1>&#39;</span><span class=p>]</span><span class=p>)</span><span class=k>print</span><span class=p>(</span><span class=n>crosstab</span><span class=p>)</span>
</code></pre></td></tr></table></div></div><p><img class=lazyload src=/svg/loading.min.svg data-src=https://cdn.jsdelivr.net/gh/QiMington/picbed/640-20231228133726791.png data-srcset="https://cdn.jsdelivr.net/gh/QiMington/picbed/640-20231228133726791.png, https://cdn.jsdelivr.net/gh/QiMington/picbed/640-20231228133726791.png 1.5x, https://cdn.jsdelivr.net/gh/QiMington/picbed/640-20231228133726791.png 2x" data-sizes=auto alt=https://cdn.jsdelivr.net/gh/QiMington/picbed/640-20231228133726791.png title=图片></p><p>最后，我们将<strong>在交叉表上运行卡方检验</strong>，这将告诉我们这两个特征是否独立。</p><pre><code>from scipy.stats import chi2_contingency
chi2_contingency(crosstab)
</code></pre><p><img class=lazyload src=/svg/loading.min.svg data-src=https://cdn.jsdelivr.net/gh/QiMington/picbed/640-20231228133731027.png data-srcset="https://cdn.jsdelivr.net/gh/QiMington/picbed/640-20231228133731027.png, https://cdn.jsdelivr.net/gh/QiMington/picbed/640-20231228133731027.png 1.5x, https://cdn.jsdelivr.net/gh/QiMington/picbed/640-20231228133731027.png 2x" data-sizes=auto alt=https://cdn.jsdelivr.net/gh/QiMington/picbed/640-20231228133731027.png title=图片></p><p>输出依次是<strong>卡方值、p 值、自由度和预期频率数组</strong>。</p><p><strong>p 值 &lt;0.05</strong>，因此我们可以<strong>拒绝特征之间没有关联的原假设</strong>，即两个特征之间存在统计上显着的关系。</p><p>由于这两个特征之间存在关联，我们可以<strong>选择删除其中一个</strong>。</p><p>到目前为止，我已经展示了在实现模型之前应用的特征选择策略。这些策略在第一轮特征选择以建立初始模型时很有用。但是一旦构建了模型，就可以获得有关模型性能中每个特征的适应度的更多信息。根据这些新信息，可以进一步确定要保留哪些功能。</p><p>下面我们使用最简单的线性模型展示其中的一些方法。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-python data-lang=python><span class=kn>from</span> <span class=nn>sklearn.model_selection</span> <span class=kn>import</span> <span class=n>train_test_splitfrom</span> <span class=n>sklearn</span><span class=o>.</span><span class=n>linear_model</span> <span class=kn>import</span> <span class=nn>LinearRegressionfrom</span> <span class=nn>sklearn.preprocessing</span> <span class=nn>import</span> <span class=nn>StandardScaler</span>
<span class=c1># drop columns with missing valuesdf = df.dropna()</span>
<span class=c1># get dummies for categorical featuresdf = pd.get_dummies(df, drop_first=True)</span>
<span class=c1># X featuresX = df.drop(&#39;price&#39;, axis=1)</span>
<span class=c1># y targety = df[&#39;price&#39;]</span>
<span class=c1># split data into training and testing setX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)</span>
<span class=n>scaler</span> <span class=o>=</span> <span class=n>StandardScaler</span><span class=p>(</span><span class=p>)</span><span class=n>X_train</span> <span class=o>=</span> <span class=n>scaler</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>X_train</span><span class=p>)</span><span class=n>X_test</span> <span class=o>=</span> <span class=n>scaler</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>X_test</span><span class=p>)</span>
<span class=c1># convert back to dataframeX_train = pd.DataFrame(X_train, columns = X.columns.to_list())X_test = pd.DataFrame(X_test, columns = X.columns.to_list())# instantiate modelmodel = LinearRegression()# fitmodel.fit(X_train, y_train)</span>
</code></pre></td></tr></table></div></div><p>现在我们已经拟合了模型，让我们进行另一轮特征选择。</p><h2 id=6-特征系数><strong>6 特征系数</strong></h2><p>如果正在运行<strong>回归任务</strong>，则特征适应度的一个关键指标是<strong>回归系数</strong>（所谓的 beta 系数），它显示了<strong>模型中特征的相对贡献</strong>。有了这些信息，可以<strong>删除贡献很小或没有贡献</strong>的功能。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-python data-lang=python><span class=c1># feature coefficientscoeffs = model.coef_</span>
<span class=c1># visualizing coefficientsindex = X_train.columns.tolist()</span>
<span class=p>(</span><span class=n>pd</span><span class=o>.</span><span class=n>DataFrame</span><span class=p>(</span><span class=n>coeffs</span><span class=p>,</span> <span class=n>index</span> <span class=o>=</span> <span class=n>index</span><span class=p>,</span> <span class=n>columns</span> <span class=o>=</span> <span class=p>[</span><span class=sa></span><span class=s1>&#39;</span><span class=s1>coeff</span><span class=s1>&#39;</span><span class=p>]</span><span class=p>)</span><span class=o>.</span><span class=n>sort_values</span><span class=p>(</span><span class=n>by</span> <span class=o>=</span> <span class=sa></span><span class=s1>&#39;</span><span class=s1>coeff</span><span class=s1>&#39;</span><span class=p>)</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>kind</span><span class=o>=</span><span class=sa></span><span class=s1>&#39;</span><span class=s1>barh</span><span class=s1>&#39;</span><span class=p>,</span> <span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>4</span><span class=p>,</span><span class=mi>10</span><span class=p>)</span><span class=p>)</span><span class=p>)</span>
</code></pre></td></tr></table></div></div><p><img class=lazyload src=/svg/loading.min.svg data-src=https://cdn.jsdelivr.net/gh/QiMington/picbed/640-20231228133737702.png data-srcset="https://cdn.jsdelivr.net/gh/QiMington/picbed/640-20231228133737702.png, https://cdn.jsdelivr.net/gh/QiMington/picbed/640-20231228133737702.png 1.5x, https://cdn.jsdelivr.net/gh/QiMington/picbed/640-20231228133737702.png 2x" data-sizes=auto alt=https://cdn.jsdelivr.net/gh/QiMington/picbed/640-20231228133737702.png title=图片></p><p>某些特征 <strong>beta 系数很小</strong>，对汽车价格的预测贡献不大。可以过滤掉这些特征：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-python data-lang=python><span class=c1># filter variables near zero coefficient valuetemp = pd.DataFrame(coeffs, index = index, columns = [&#39;coeff&#39;]).sort_values(by = &#39;coeff&#39;)temp = temp[(temp[&#39;coeff&#39;]&gt;1) | (temp[&#39;coeff&#39;]&lt; -1)]</span>
<span class=c1># drop those featurescols_coeff = temp.index.to_list()X_train = X_train[cols_coeff]X_test = X_test[cols_coeff]</span>
</code></pre></td></tr></table></div></div><h2 id=7-p-值><strong>7 p 值</strong></h2><p>在回归中，p 值告诉我们<strong>预测变量和目标之间</strong>的关系<strong>是否具有统计显著性</strong>。statsmodels 库提供了带有特征系数和相关 p 值的回归输出的函数。</p><p>如果某些特征不显著，可以<strong>将它们一个一个移除</strong>，然后每次重新运行模型，<strong>直到找到一组具有显着 p 值的特征</strong>，并通过更高的调整 R2 提高性能。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-python data-lang=python><span class=kn>import</span> <span class=nn>statsmodels.api</span> <span class=kn>as</span> <span class=nn>smols</span> <span class=o>=</span> <span class=n>sm</span><span class=o>.</span><span class=n>OLS</span><span class=p>(</span><span class=n>y</span><span class=p>,</span> <span class=n>X</span><span class=p>)</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=p>)</span><span class=k>print</span><span class=p>(</span><span class=n>ols</span><span class=o>.</span><span class=n>summary</span><span class=p>(</span><span class=p>)</span><span class=p>)</span>
</code></pre></td></tr></table></div></div><p><img class=lazyload src=/svg/loading.min.svg data-src=https://cdn.jsdelivr.net/gh/QiMington/picbed/640-20231228133746421.png data-srcset="https://cdn.jsdelivr.net/gh/QiMington/picbed/640-20231228133746421.png, https://cdn.jsdelivr.net/gh/QiMington/picbed/640-20231228133746421.png 1.5x, https://cdn.jsdelivr.net/gh/QiMington/picbed/640-20231228133746421.png 2x" data-sizes=auto alt=https://cdn.jsdelivr.net/gh/QiMington/picbed/640-20231228133746421.png title=图片></p><h2 id=8-方差膨胀因子-vif><strong>8 方差膨胀因子 (VIF)</strong></h2><p>方差膨胀因子 (VIF) 是衡量<strong>多重共线性</strong>的另一种方法。它被<strong>测量</strong>为<strong>整体模型方差与每个独立特征的方差的比率</strong>。一个特征的<strong>高 VIF</strong> 表明它<strong>与一个或多个其他特征相关</strong>。根据经验：</p><ul><li><strong>VIF = 1</strong> 表示<strong>无相关</strong>性</li><li><strong>VIF = 1-5</strong> <strong>中等相关</strong>性</li><li><strong>VIF >5 高相关</strong></li></ul><p>VIF 是一种消除多重共线性特征的有用技术。对于我们的演示，<strong>将所有 VIF 高于10的删除</strong>。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-python data-lang=python><span class=kn>from</span> <span class=nn>statsmodels.stats.outliers_influence</span> <span class=kn>import</span> <span class=n>variance_inflation_factor</span>
<span class=c1># calculate VIFvif = pd.Series([variance_inflation_factor(X.values, i) for i in range(X.shape[1])], index=X.columns)</span>
<span class=c1># display VIFs in a tableindex = X_train.columns.tolist()vif_df = pd.DataFrame(vif, index = index, columns = [&#39;vif&#39;]).sort_values(by = &#39;vif&#39;, ascending=False)vif_df[vif_df[&#39;vif&#39;]&lt;10]</span>
</code></pre></td></tr></table></div></div><p><img class=lazyload src=/svg/loading.min.svg data-src=https://cdn.jsdelivr.net/gh/QiMington/picbed/640-20231228133752395-20231228133757691.png data-srcset="https://cdn.jsdelivr.net/gh/QiMington/picbed/640-20231228133752395-20231228133757691.png, https://cdn.jsdelivr.net/gh/QiMington/picbed/640-20231228133752395-20231228133757691.png 1.5x, https://cdn.jsdelivr.net/gh/QiMington/picbed/640-20231228133752395-20231228133757691.png 2x" data-sizes=auto alt=https://cdn.jsdelivr.net/gh/QiMington/picbed/640-20231228133752395-20231228133757691.png title=图片></p><h2 id=9-基于特征重要性选择><strong>9 基于特征重要性选择</strong></h2><p>决策树/随机森林<strong>使用一个特征来分割数据</strong>，该特征最大程度地减少了杂质(以基尼系数杂质或信息增益衡量)。找到最佳特征是算法如何在分类任务中工作的关键部分。我们可以通过 **feature_importances_** 属性访问最好的特征。</p><p>让我们在我们的数据集上实现一个随机森林模型并过滤一些特征。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-python data-lang=python><span class=kn>from</span> <span class=nn>sklearn.ensemble</span> <span class=kn>import</span> <span class=n>RandomForestClassifier</span>
<span class=c1># instantiate modelmodel = RandomForestClassifier(n_estimators=200, random_state=0)# fit modelmodel.fit(X,y)</span>

</code></pre></td></tr></table></div></div><p>现在让我们看看特征重要性：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-python data-lang=python><span class=c1># feature importanceimportances = model.feature_importances_</span>
<span class=c1># visualizationcols = X.columns(pd.DataFrame(importances, cols, columns = [&#39;importance&#39;]).sort_values(by=&#39;importance&#39;, ascending=True).plot(kind=&#39;barh&#39;, figsize=(4,10)))</span>
</code></pre></td></tr></table></div></div><p><img class=lazyload src=/svg/loading.min.svg data-src=https://cdn.jsdelivr.net/gh/QiMington/picbed/640-20231228133803779.png data-srcset="https://cdn.jsdelivr.net/gh/QiMington/picbed/640-20231228133803779.png, https://cdn.jsdelivr.net/gh/QiMington/picbed/640-20231228133803779.png 1.5x, https://cdn.jsdelivr.net/gh/QiMington/picbed/640-20231228133803779.png 2x" data-sizes=auto alt=https://cdn.jsdelivr.net/gh/QiMington/picbed/640-20231228133803779.png title=图片></p><p>上面的输出<strong>显示了每个特征在减少每个节点/拆分处的重要性</strong>。</p><p>由于随机森林分类器有很多估计量（例如上面例子中的 200 棵决策树），可以<strong>用置信区间计算相对重要性</strong>的估计值。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-python data-lang=python><span class=c1># calculate standard deviation of feature importancesstd = np.std([i.feature_importances_ for i in model.estimators_], axis=0)</span>
<span class=c1># visualizationfeat_with_importance = pd.Series(importances, X.columns)fig, ax = plt.subplots(figsize=(12,5))feat_with_importance.plot.bar(yerr=std, ax=ax)ax.set_title(&#34;Feature importances&#34;)ax.set_ylabel(&#34;Mean decrease in impurity&#34;)</span>
</code></pre></td></tr></table></div></div><p><img class=lazyload src=/svg/loading.min.svg data-src=https://cdn.jsdelivr.net/gh/QiMington/picbed/640-20231228133808500.png data-srcset="https://cdn.jsdelivr.net/gh/QiMington/picbed/640-20231228133808500.png, https://cdn.jsdelivr.net/gh/QiMington/picbed/640-20231228133808500.png 1.5x, https://cdn.jsdelivr.net/gh/QiMington/picbed/640-20231228133808500.png 2x" data-sizes=auto alt=https://cdn.jsdelivr.net/gh/QiMington/picbed/640-20231228133808500.png title=图片></p><p>现在我们知道了每个特征的重要性，可以手动（或以编程方式）确定保留哪些特征以及删除哪些特征。</p><h2 id=10-使用-scikit-learn-自动选择特征><strong>10 使用 Scikit Learn 自动选择特征</strong></h2><p>sklearn 库中有一个完整的模块，只需几行代码即可处理特征选择。</p><p>sklearn 中有许多自动化流程，但这里我只展示一些：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-python data-lang=python><span class=c1># import modulesfrom sklearn.feature_selection import (SelectKBest, chi2, SelectPercentile, SelectFromModel,                                        SequentialFeatureSelector, SequentialFeatureSelector)</span>
</code></pre></td></tr></table></div></div><p><strong>1) 基于卡方的技术</strong></p><p>基于卡方的技术<strong>根据一些预定义的分数选择特定数量的用户定义特征</strong> (k)。这些分数是<strong>通过计算 X（独立）和 y（因）变量之间的卡方统计量来确定</strong>的。在 sklearn 中，需要做的就是确定要保留多少特征。如果想保留 10 个功能，实现将如下所示：</p><pre><code># select K best featuresX_best = SelectKBest(chi2, k=10).fit_transform(X,y)
# number of best featuresX_best.shape[1] # 10
</code></pre><p>如果有大量特征，可以指定要保留的特征百分比。假设我们想要<strong>保留 75% 的特征并丢弃剩余的 25%</strong>：</p><pre><code># keep 75% top featuresX_top = SelectPercentile(chi2, percentile = 75).fit_transform(X,y)
# number of best featuresX_top.shape[1] # 36
</code></pre><p><strong>2) 正则化</strong></p><p>正则化<strong>减少了过拟合</strong>。如果你有太多的特征，正则化控制它们的效果，或者通过<strong>缩小特征系数</strong>（称为 L2 正则化）或<strong>将一些特征系数设置为零</strong>（称为 L1 正则化）。</p><p>一些模型具有内置的 L1/L2 正则化<strong>作为超参数来惩罚特征</strong>。可以使用转换器 SelectFromModel 消除这些功能。</p><p>让我们实现一个带有惩罚 = &lsquo;l1&rsquo; 的 <a href="http://mp.weixin.qq.com/s?__biz=MzI4ODgwMjYyNQ==&mid=2247484224&idx=1&sn=bdcacc9cccc6d29baaf9ef84867c0d6a&chksm=ec39916edb4e187830e158c58c76aa9694da949342ed22a90c287cc3fff71ac702a6e2f0cbec&scene=21#wechat_redirect" target=_blank rel="noopener noreffer"><strong>LinearSVC 算法</strong></a>
。然后使用 <strong>SelectFromModel</strong> 删除一些功能。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-python data-lang=python><span class=c1># implement algorithmfrom sklearn.svm import LinearSVCmodel = LinearSVC(penalty= &#39;l1&#39;, C = 0.002, dual=False)model.fit(X,y)# select features using the meta transformerselector = SelectFromModel(estimator = model, prefit=True)</span>
<span class=n>X_new</span> <span class=o>=</span> <span class=n>selector</span><span class=o>.</span><span class=n>transform</span><span class=p>(</span><span class=n>X</span><span class=p>)</span><span class=n>X_new</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span> <span class=c1># 2</span>
<span class=c1># names of selected featuresfeature_names = np.array(X.columns)feature_names[selector.get_support()]# array([&#39;wheel-base&#39;, &#39;horsepower&#39;], dtype=object)</span>
</code></pre></td></tr></table></div></div><p><strong>3) 序贯法</strong></p><p>序贯法是一种经典的<strong>统计技术</strong>。在这种情况<strong>下一次添加/删除</strong>一个功能并检查模型性能，直到它针对需求进行优化。</p><p>序贯法有<strong>两种变体</strong>。<strong>前向选择技术从 0 特征开始</strong>，然后<strong>添加</strong>一个最大程度地减少错误的特征；然后添加另一个特征，依此类推。</p><p><strong>向后选择在相反的方向上起作用</strong>。模型从包含的所有特征开始并计算误差；然后它<strong>消除</strong>了一个可以进一步减少误差的特征。重复该过程，直到保留所需数量的特征。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-python data-lang=python><span class=c1># instantiate modelmodel = RandomForestClassifier(n_estimators=100, random_state=0)</span>
<span class=c1># select featuresselector = SequentialFeatureSelector(estimator=model, n_features_to_select=10, direction=&#39;backward&#39;, cv=2)selector.fit_transform(X,y)</span>
<span class=c1># check names of features selectedfeature_names = np.array(X.columns)feature_names[selector.get_support()]</span>
</code></pre></td></tr></table></div></div><p><img class=lazyload src=/svg/loading.min.svg data-src=https://cdn.jsdelivr.net/gh/QiMington/picbed/640-20231228133823754.png data-srcset="https://cdn.jsdelivr.net/gh/QiMington/picbed/640-20231228133823754.png, https://cdn.jsdelivr.net/gh/QiMington/picbed/640-20231228133823754.png 1.5x, https://cdn.jsdelivr.net/gh/QiMington/picbed/640-20231228133823754.png 2x" data-sizes=auto alt=https://cdn.jsdelivr.net/gh/QiMington/picbed/640-20231228133823754.png title=图片></p><p><strong>11 主成分分析 (PCA)</strong></p><p><a href="http://mp.weixin.qq.com/s?__biz=MzI4ODgwMjYyNQ==&mid=2247483848&idx=1&sn=7cfd6d06e94e4a85abf276cf345c15e4&chksm=ec3993e6db4e1af068abc956f5b61e4e933ffe1779394452228f284afa251ebbb36db156ae18&scene=21#wechat_redirect" target=_blank rel="noopener noreffer"><strong>PCA</strong></a>
的主要目的是<strong>降低高维特征空间的维数</strong>。原始特征被重新投影到新的维度（即主成分）。最终目标是找到<strong>最能解释数据方差的特征数量</strong>。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre class=chroma><code class=language-python data-lang=python><span class=c1># import PCA modulefrom sklearn.decomposition import PCA# scaling dataX_scaled = scaler.fit_transform(X)# fit PCA to datapca = PCA()pca.fit(X_scaled)evr = pca.explained_variance_ratio_</span>
<span class=c1># visualizing the variance explained by each principal componentsplt.figure(figsize=(12, 5))plt.plot(range(0, len(evr)), evr.cumsum(), marker=&#34;o&#34;, linestyle=&#34;--&#34;)plt.xlabel(&#34;Number of components&#34;)plt.ylabel(&#34;Cumulative explained variance&#34;)</span>
</code></pre></td></tr></table></div></div><p><img class=lazyload src=/svg/loading.min.svg data-src=https://cdn.jsdelivr.net/gh/QiMington/picbed/640-20231228133829238.png data-srcset="https://cdn.jsdelivr.net/gh/QiMington/picbed/640-20231228133829238.png, https://cdn.jsdelivr.net/gh/QiMington/picbed/640-20231228133829238.png 1.5x, https://cdn.jsdelivr.net/gh/QiMington/picbed/640-20231228133829238.png 2x" data-sizes=auto alt=https://cdn.jsdelivr.net/gh/QiMington/picbed/640-20231228133829238.png title=图片></p><p><strong>20 个主成分解释了超过 80% 的方差</strong>，因此可以将模型拟合到这 20 个成分（特征）。可以预先确定<strong>方差阈值</strong>并选择所需的主成分数量。</p><h2 id=总结><strong>总结</strong></h2><p>这是对可应用于特征选择的各种技术的有用指南。在拟合模型之前应用了一些技术，例如删除具有缺失值的列、不相关的列、具有多重共线性的列以及使用 PCA 进行降维，而在基本模型实现之后应用其他技术，例如特征系数、p 值、 VIF 等。虽然不会在一个项目中完全使用所有策略，这些策略都是我们进行测试的方向。</p><p>本文代码：https://github.com/mabalam/feature_selection</p><p>作者：Mahbubul Alam</p><p>转自：DeepHub IMBA</p><p>文章链接https://mp.weixin.qq.com/s/Ibi37luBTwlRrVGemXS_EA</p></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span>更新于 2022-05-20</span></div></div><div class=post-info-line><div class=post-info-md></div><div class=post-info-share><span></span></div></div></div><div class=post-info-more><section class=post-tags><i class="fas fa-tags fa-fw" aria-hidden=true></i>&nbsp;<a href=/tags/python/>Python</a>,&nbsp;<a href=/tags/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/>特征选择</a></section><section><span><a href=javascript:void(0); onclick=window.history.back();>返回</a></span>&nbsp;|&nbsp;<span><a href=/>主页</a></span></section></div><div class=post-nav><a href=/posts/redis/redis%E5%85%A5%E9%97%A8/ class=next rel=next title=Redis入门>Redis入门<i class="fas fa-angle-right fa-fw" aria-hidden=true></i></a></div></div></article></div></main><footer class=footer><div class=footer-container><div class=footer-line><i class="far fa-copyright fa-fw" aria-hidden=true></i><span itemprop=copyrightYear>2022 - 2025</span><span class=author itemprop=copyrightHolder>&nbsp;<a href=/ target=_blank>QiMington</a></span></div><div class=footer-line itemscope itemtype=http://schema.org/CreativeWork><span class=license><img src=/beian.png style=vertical-align:middle;height:16.5px><a href="https://beian.mps.gov.cn/#/query/webSearch?code=42088102000171" rel=noreferrer target=_blank>鄂公网安备42088102000171</a></span><span class=icp-splitter>&nbsp;&nbsp;|&nbsp;</span><br class=icp-br><span class=icp><a href=https://beian.miit.gov.cn target=_blank>鄂ICP备2022006388号</a></span></div></div></footer></div><div id=fixed-buttons><a href=# id=back-to-top class=fixed-button title=回到顶部><i class="fas fa-arrow-up fa-fw" aria-hidden=true></i></a><a href=# id=view-comments class=fixed-button title=查看评论><i class="fas fa-comment fa-fw" aria-hidden=true></i></a></div><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/contrib/copy-tex.min.css><script type=text/javascript src=https://cdn.jsdelivr.net/npm/lazysizes@5.3.1/lazysizes.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/contrib/auto-render.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/contrib/copy-tex.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/contrib/mhchem.min.js></script><script type=text/javascript>window.config={"code":{"copyTitle":"复制到剪贴板","maxShownLines":50},"comment":{},"math":{"delimiters":[{"display":true,"left":"$$","right":"$$"},{"display":true,"left":"\\[","right":"\\]"},{"display":true,"left":"\\begin{equation}","right":"\\end{equation}"},{"display":true,"left":"\\begin{equation*}","right":"\\end{equation*}"},{"display":true,"left":"\\begin{align}","right":"\\end{align}"},{"display":true,"left":"\\begin{align*}","right":"\\end{align*}"},{"display":true,"left":"\\begin{alignat}","right":"\\end{alignat}"},{"display":true,"left":"\\begin{alignat*}","right":"\\end{alignat*}"},{"display":true,"left":"\\begin{gather}","right":"\\end{gather}"},{"display":true,"left":"\\begin{CD}","right":"\\end{CD}"},{"display":false,"left":"$","right":"$"},{"display":false,"left":"\\(","right":"\\)"}],"strict":false}};</script><script type=text/javascript src=/js/theme.min.js></script></body></html>